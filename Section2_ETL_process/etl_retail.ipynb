{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d9b64a3",
   "metadata": {},
   "source": [
    "# Task 2: ETL Process Implementation\n",
    "\n",
    "This task involves implementing a full ETL (Extract, Transform, Load) pipeline for a retail dataset.\n",
    "\n",
    "We will generate synthetic data that mimics the \"Online Retail\" dataset structure to simulate real-world retail transactions. Then we will perform the following ETL steps:\n",
    "\n",
    "- **Extract:** Load or generate the dataset, ensuring correct data types and handling missing values.\n",
    "- **Transform:** Create new calculated columns, filter recent data, summarize customer data, and clean outliers.\n",
    "- **Load:** Insert the transformed data into an SQLite database structured with fact and dimension tables.\n",
    "\n",
    "Each step will be clearly explained, documented, and logged to ensure robustness and clarity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c24603a",
   "metadata": {},
   "source": [
    "# Step 0: Synthetic Data Generation\n",
    "\n",
    "### Step 0: Synthetic Data Generation with Real-World Imperfections\n",
    "\n",
    "In this step, we generate a synthetic retail dataset (~1100 rows) designed to resemble real-world data by including:\n",
    "\n",
    "- Valid sales records with random realistic values.\n",
    "- Outliers: negative or zero quantities and unit prices to simulate data entry errors.\n",
    "- Duplicate records to represent accidental repeated entries.\n",
    "- Missing values in some columns to mimic incomplete data.\n",
    "\n",
    "After generation, the dataset is saved as `synthetic_retail_data.csv` to simulate an extraction source file for the ETL process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4055e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic dataset generated and saved as 'synthetic_retail_data.csv'.\n",
      "Dataset shape: (1010, 8)\n",
      "Number of duplicates: 10\n",
      "Number of missing CustomerID: 20\n",
      "Number of missing Description: 15\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from faker import Faker\n",
    "\n",
    "# Initialize Faker for realistic data generation\n",
    "fake = Faker()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define parameters\n",
    "num_rows = 1000\n",
    "num_customers = 100\n",
    "num_countries = 7\n",
    "\n",
    "# Generate InvoiceNo as unique invoice strings\n",
    "invoice_numbers = [f\"INV{10000 + i}\" for i in range(num_rows)]\n",
    "\n",
    "# Generate StockCode as product codes\n",
    "stock_codes = [f\"P{random.randint(1000,9999)}\" for _ in range(num_rows)]\n",
    "\n",
    "# Generate product descriptions\n",
    "products = [fake.word().capitalize() + \" \" + fake.word().capitalize() for _ in range(num_rows)]\n",
    "\n",
    "# Quantities: integers 1 to 50, with some negative values as outliers\n",
    "quantities = np.random.randint(1, 51, size=num_rows)\n",
    "outlier_indices = np.random.choice(num_rows, size=10, replace=False)\n",
    "quantities[outlier_indices] = -np.random.randint(1, 20, size=10)  # negative quantities as outliers\n",
    "\n",
    "# UnitPrice: floats between 1 and 100, with some zero or negative outliers\n",
    "unit_prices = np.round(np.random.uniform(1, 100, size=num_rows), 2)\n",
    "price_outlier_indices = np.random.choice(num_rows, size=5, replace=False)\n",
    "unit_prices[price_outlier_indices] = np.random.uniform(-20, 0, size=5)  # negative or zero prices as outliers\n",
    "\n",
    "# InvoiceDate: random dates over 2 years (Aug 12, 2023 to Aug 12, 2025)\n",
    "start_date = pd.Timestamp('2023-08-12')\n",
    "end_date = pd.Timestamp('2025-08-12')\n",
    "invoice_dates = [fake.date_time_between(start_date, end_date) for _ in range(num_rows)]\n",
    "\n",
    "# CustomerID: Use float type array to allow NaNs\n",
    "customer_ids = np.random.choice(range(10000, 10000 + num_customers), size=num_rows).astype(float)\n",
    "\n",
    "# Country: random selection from 7 countries\n",
    "countries_list = ['United Kingdom', 'France', 'Germany', 'Netherlands', 'USA', 'Canada', 'Australia']\n",
    "countries = np.random.choice(countries_list, size=num_rows)\n",
    "\n",
    "# Introduce some missing CustomerID and Description values randomly\n",
    "missing_customer_indices = np.random.choice(num_rows, size=20, replace=False)\n",
    "for i in missing_customer_indices:\n",
    "    customer_ids[i] = np.nan  # Now valid as customer_ids are floats\n",
    "\n",
    "missing_description_indices = np.random.choice(num_rows, size=15, replace=False)\n",
    "for i in missing_description_indices:\n",
    "    products[i] = None\n",
    "\n",
    "# Create DataFrame\n",
    "df_synthetic = pd.DataFrame({\n",
    "    'InvoiceNo': invoice_numbers,\n",
    "    'StockCode': stock_codes,\n",
    "    'Description': products,\n",
    "    'Quantity': quantities,\n",
    "    'InvoiceDate': invoice_dates,\n",
    "    'UnitPrice': unit_prices,\n",
    "    'CustomerID': customer_ids,\n",
    "    'Country': countries\n",
    "})\n",
    "\n",
    "# Add some duplicate rows by duplicating random samples\n",
    "duplicates = df_synthetic.sample(10, random_state=42)\n",
    "df_synthetic = pd.concat([df_synthetic, duplicates], ignore_index=True)\n",
    "\n",
    "# Shuffle the dataset\n",
    "df_synthetic = df_synthetic.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save the generated dataset to CSV for extraction stage\n",
    "df_synthetic.to_csv('synthetic_retail_data.csv', index=False)\n",
    "\n",
    "print(\"Synthetic dataset generated and saved as 'synthetic_retail_data.csv'.\")\n",
    "print(f\"Dataset shape: {df_synthetic.shape}\")\n",
    "print(f\"Number of duplicates: {df_synthetic.duplicated().sum()}\")\n",
    "print(f\"Number of missing CustomerID: {df_synthetic['CustomerID'].isna().sum()}\")\n",
    "print(f\"Number of missing Description: {df_synthetic['Description'].isna().sum()}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3030090d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:29:11,234 - INFO - Logger initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:29:11,234 - INFO - Logger initialized successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# ------------------ Logger Setup ------------------\n",
    "logger = logging.getLogger(\"ETLLogger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Prevent multiple handlers in notebooks\n",
    "if not logger.handlers:\n",
    "    # File handler\n",
    "    file_handler = logging.FileHandler(\"etl_process.log\", mode='a')  # append logs\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    # Console handler\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "logger.info(\"Logger initialized successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5d212f",
   "metadata": {},
   "source": [
    "# Step 1: Extract\n",
    "\n",
    "- In this step, we read the synthetic CSV data generated previously into a pandas DataFrame. \n",
    "- We will handle missing values and data types (convert InvoiceDate to datetime).\n",
    "- This prepares the raw data for transformation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1f3ff4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:29:30,395 - INFO - ETL Process started - Extraction step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:29:30,395 - INFO - ETL Process started - Extraction step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:29:30,419 - INFO - Dropped 35 rows due to missing CustomerID or Description\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:29:30,419 - INFO - Dropped 35 rows due to missing CustomerID or Description\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:29:30,428 - INFO - Removed 10 duplicate rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:29:30,428 - INFO - Removed 10 duplicate rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:29:30,435 - INFO - Extracted data shape after cleaning: (965, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:29:30,435 - INFO - Extracted data shape after cleaning: (965, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:29:30,444 - INFO - Missing values after cleaning:\n",
      "InvoiceNo      0\n",
      "StockCode      0\n",
      "Description    0\n",
      "Quantity       0\n",
      "InvoiceDate    0\n",
      "UnitPrice      0\n",
      "CustomerID     0\n",
      "Country        0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:29:30,444 - INFO - Missing values after cleaning:\n",
      "InvoiceNo      0\n",
      "StockCode      0\n",
      "Description    0\n",
      "Quantity       0\n",
      "InvoiceDate    0\n",
      "UnitPrice      0\n",
      "CustomerID     0\n",
      "Country        0\n",
      "dtype: int64\n",
      "  InvoiceNo StockCode        Description  Quantity         InvoiceDate  \\\n",
      "0  INV10629     P5569              It Of        45 2025-06-30 03:26:14   \n",
      "1  INV10788     P1423         Live Eight        35 2024-08-05 19:57:11   \n",
      "2  INV10684     P8933       Unit Example        45 2023-12-01 22:12:30   \n",
      "3  INV10516     P2020  Fill Relationship         1 2024-12-12 00:34:30   \n",
      "4  INV10529     P9947       Though Could        24 2024-03-12 10:59:29   \n",
      "\n",
      "   UnitPrice  CustomerID         Country  \n",
      "0       5.41       10089  United Kingdom  \n",
      "1      74.23       10062          Canada  \n",
      "2      66.75       10063       Australia  \n",
      "3      56.25       10034     Netherlands  \n",
      "4      38.15       10023       Australia  \n"
     ]
    }
   ],
   "source": [
    "logger.info(\"ETL Process started - Extraction step\")\n",
    "\n",
    "# Load synthetic CSV dataset\n",
    "df = pd.read_csv('synthetic_retail_data.csv')\n",
    "\n",
    "# Convert InvoiceDate to datetime format for time-based operations\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='coerce')\n",
    "\n",
    "# Drop rows where critical columns have missing values\n",
    "missing_before = df.shape[0]\n",
    "df = df.dropna(subset=['CustomerID', 'Description'])\n",
    "missing_after = df.shape[0]\n",
    "logger.info(f\"Dropped {missing_before - missing_after} rows due to missing CustomerID or Description\")\n",
    "\n",
    "# Remove duplicate rows\n",
    "duplicates_before = df.shape[0]\n",
    "df = df.drop_duplicates()\n",
    "duplicates_after = df.shape[0]\n",
    "logger.info(f\"Removed {duplicates_before - duplicates_after} duplicate rows\")\n",
    "\n",
    "# Ensure proper data types\n",
    "df['Quantity'] = pd.to_numeric(df['Quantity'], errors='coerce').fillna(0).astype(int)\n",
    "df['UnitPrice'] = pd.to_numeric(df['UnitPrice'], errors='coerce').fillna(0.0).astype(float)\n",
    "df['CustomerID'] = pd.to_numeric(df['CustomerID'], errors='coerce').astype('Int64')\n",
    "\n",
    "logger.info(f\"Extracted data shape after cleaning: {df.shape}\")\n",
    "logger.info(f\"Missing values after cleaning:\\n{df.isna().sum()}\")\n",
    "\n",
    "# Optional: show first 5 rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf5af7b",
   "metadata": {},
   "source": [
    "# Output Analysis:\n",
    "\n",
    "-  The dataset was successfully loaded from the CSV file.\n",
    "-  InvoiceDate was converted to datetime format for easier filtering later.\n",
    "-  The dataset contains missing CustomerID and Description values, as expected from the synthetic generation.\n",
    "-  There are also duplicate rows, which will be handled during transformation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ed312f",
   "metadata": {},
   "source": [
    "# Step 2 : Transform\n",
    "\n",
    "In this step, we perform the following transformations on the extracted data:\n",
    "\n",
    "1. Calculate a new column `TotalSales` as Quantity multiplied by UnitPrice.\n",
    "2. Create a customer summary by grouping data by CustomerID, aggregating total purchases and capturing associated country.\n",
    "3. Filter sales to only include data from the last year (August 13, 2024 to August 12, 2025).\n",
    "4. Remove outliers by filtering out rows where Quantity is less than 0 or UnitPrice is less than or equal to 0.\n",
    "\n",
    "***These steps ensure the dataset is clean, relevant, and ready for loading.***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf0d7357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:30:06,094 - INFO - Transformation step started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:30:06,094 - INFO - Transformation step started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:30:06,100 - INFO - Calculated TotalSales column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:30:06,100 - INFO - Calculated TotalSales column\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:30:06,106 - INFO - Removed outliers: 15 rows dropped due to invalid Quantity or UnitPrice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:30:06,106 - INFO - Removed outliers: 15 rows dropped due to invalid Quantity or UnitPrice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:30:06,116 - INFO - Filtered data for sales in last year: data shape now (476, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:30:06,116 - INFO - Filtered data for sales in last year: data shape now (476, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:30:06,207 - INFO - Created customer summary with 100 unique customers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:30:06,207 - INFO - Created customer summary with 100 unique customers\n",
      "  InvoiceNo StockCode        Description  Quantity         InvoiceDate  \\\n",
      "0  INV10629     P5569              It Of        45 2025-06-30 03:26:14   \n",
      "3  INV10516     P2020  Fill Relationship         1 2024-12-12 00:34:30   \n",
      "5  INV10657     P1282     Occur Evidence         5 2025-01-19 03:15:43   \n",
      "6  INV10552     P1823        What Follow        35 2025-02-14 15:01:34   \n",
      "8  INV10321     P5533         Push Start        14 2024-10-21 19:53:14   \n",
      "\n",
      "   UnitPrice  CustomerID         Country  TotalSales  \n",
      "0       5.41       10089  United Kingdom      243.45  \n",
      "3      56.25       10034     Netherlands       56.25  \n",
      "5      49.95       10006          France      249.75  \n",
      "6       3.88       10060          France      135.80  \n",
      "8      32.44       10065          France      454.16  \n",
      "   CustomerID  TotalPurchases    Country\n",
      "0       10000         2074.73     Canada\n",
      "1       10001         1522.08     Canada\n",
      "2       10002         2569.71    Germany\n",
      "3       10003         8640.77     France\n",
      "4       10004         9295.60  Australia\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Transformation step started\")\n",
    "\n",
    "# Step 1: Calculate TotalSales column\n",
    "df['TotalSales'] = df['Quantity'] * df['UnitPrice']\n",
    "logger.info(\"Calculated TotalSales column\")\n",
    "\n",
    "# Step 2: Handle outliers - Remove invalid rows\n",
    "initial_shape = df.shape\n",
    "df = df[(df['Quantity'] >= 0) & (df['UnitPrice'] > 0)]\n",
    "logger.info(f\"Removed outliers: {initial_shape[0] - df.shape[0]} rows dropped due to invalid Quantity or UnitPrice\")\n",
    "\n",
    "# Step 3: Filter for sales in the last year\n",
    "start_date = pd.to_datetime('2024-08-13')\n",
    "end_date = pd.to_datetime('2025-08-12')\n",
    "df = df[(df['InvoiceDate'] >= start_date) & (df['InvoiceDate'] <= end_date)]\n",
    "logger.info(f\"Filtered data for sales in last year: data shape now {df.shape}\")\n",
    "\n",
    "# Step 4: Create customer summary\n",
    "customer_summary = df.groupby('CustomerID').agg(\n",
    "    TotalPurchases=pd.NamedAgg(column='TotalSales', aggfunc='sum'),\n",
    "    Country=pd.NamedAgg(column='Country', aggfunc=lambda x: x.mode()[0] if not x.mode().empty else 'Unknown')\n",
    ").reset_index()\n",
    "logger.info(f\"Created customer summary with {customer_summary.shape[0]} unique customers\")\n",
    "\n",
    "# Optional: show first 5 rows\n",
    "print(df.head())\n",
    "print(customer_summary.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1116ec03",
   "metadata": {},
   "source": [
    "## Step 2.1: Save Transformed Data to CSV\n",
    "- After cleaning and transforming the data (calculating TotalSales, filtering, etc.),\n",
    "- we save the resulting DataFrame to a CSV file named 'transformed_retail_data.csv'.\n",
    "- This allows for easy data sharing and serves as an intermediate checkpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f73749a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:30:25,482 - INFO - Transformed data saved to transformed_retail_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:30:25,482 - INFO - Transformed data saved to transformed_retail_data.csv\n"
     ]
    }
   ],
   "source": [
    "transformed_csv_path = 'transformed_retail_data.csv'\n",
    "\n",
    "# Save transformed DataFrame to CSV without the index column\n",
    "df.to_csv(transformed_csv_path, index=False)\n",
    "logger.info(f\"Transformed data saved to {transformed_csv_path}\")\n",
    "# Read the transformed data back to verify\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085c7184",
   "metadata": {},
   "source": [
    "## Save Output Analysis:\n",
    "\n",
    "- The transformed sales data has been saved successfully to 'transformed_retail_data.csv'.\n",
    "- This file contains all cleaned, filtered, and enhanced records, ready for loading into the data warehouse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b029666a",
   "metadata": {},
   "source": [
    "# Step 3: Load (Inserting Data into SQLite Database)\n",
    "\n",
    "In this step, we will load the transformed retail data into a SQLite database named `retail_dw.db`.\n",
    "\n",
    "We will create three tables:\n",
    "- `CustomerDim` to store unique customers,\n",
    "- `TimeDim` to store unique dates and time attributes,\n",
    "- `SalesFact` to store the sales transactions linked to the customer and time dimensions via foreign keys.\n",
    "\n",
    "This design follows the data warehousing star schema pattern and satisfies the project requirement to load data into a fact table and at least two dimension tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e2dbc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:30:42,365 - INFO - Load step started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:30:42,365 - INFO - Load step started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:30:42,374 - INFO - Tables created or verified\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:30:42,374 - INFO - Tables created or verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:30:42,440 - INFO - Inserted 343 customers into CustomerDim\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:30:42,440 - INFO - Inserted 343 customers into CustomerDim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:30:42,653 - INFO - Inserted 476 dates into TimeDim\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:30:42,653 - INFO - Inserted 476 dates into TimeDim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:30:42,900 - INFO - Inserted 476 sales records into SalesFact\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:30:42,900 - INFO - Inserted 476 sales records into SalesFact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:30:42,909 - INFO - Load step completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table CustomerDim has 100 records.\n",
      "Table TimeDim has 351 records.\n",
      "Table SalesFact has 3302 records.\n",
      "2025-08-13 09:30:42,909 - INFO - Load step completed\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Logger assumed already configured at the start of your script\n",
    "\n",
    "logger.info(\"Load step started\")\n",
    "\n",
    "# Connect or create SQLite DB\n",
    "conn = sqlite3.connect('retail_dw.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create dimension and fact tables if not exist\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS CustomerDim (\n",
    "    customer_id INTEGER PRIMARY KEY,\n",
    "    country TEXT\n",
    ")\n",
    "''')\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS TimeDim (\n",
    "    time_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    date TEXT UNIQUE,\n",
    "    year INTEGER,\n",
    "    month INTEGER,\n",
    "    day INTEGER\n",
    ")\n",
    "''')\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS SalesFact (\n",
    "    sales_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    invoice_no TEXT,\n",
    "    stock_code TEXT,\n",
    "    description TEXT,\n",
    "    quantity INTEGER,\n",
    "    unit_price REAL,\n",
    "    total_sales REAL,\n",
    "    customer_id INTEGER,\n",
    "    time_id INTEGER,\n",
    "    FOREIGN KEY(customer_id) REFERENCES CustomerDim(customer_id),\n",
    "    FOREIGN KEY(time_id) REFERENCES TimeDim(time_id)\n",
    ")\n",
    "''')\n",
    "\n",
    "conn.commit()\n",
    "logger.info(\"Tables created or verified\")\n",
    "\n",
    "# Insert unique customers into CustomerDim\n",
    "customers = df[['CustomerID', 'Country']].drop_duplicates().dropna()\n",
    "customers.columns = ['customer_id', 'country']\n",
    "\n",
    "for _, row in customers.iterrows():\n",
    "    cursor.execute('''\n",
    "        INSERT OR IGNORE INTO CustomerDim (customer_id, country) VALUES (?, ?)\n",
    "    ''', (int(row['customer_id']), row['country']))\n",
    "conn.commit()\n",
    "logger.info(f\"Inserted {customers.shape[0]} customers into CustomerDim\")\n",
    "\n",
    "# Insert unique dates into TimeDim\n",
    "dates = df[['InvoiceDate']].drop_duplicates()\n",
    "dates['year'] = dates['InvoiceDate'].dt.year\n",
    "dates['month'] = dates['InvoiceDate'].dt.month\n",
    "dates['day'] = dates['InvoiceDate'].dt.day\n",
    "dates['date_str'] = dates['InvoiceDate'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "for _, row in dates.iterrows():\n",
    "    cursor.execute('''\n",
    "        INSERT OR IGNORE INTO TimeDim (date, year, month, day) VALUES (?, ?, ?, ?)\n",
    "    ''', (row['date_str'], row['year'], row['month'], row['day']))\n",
    "conn.commit()\n",
    "logger.info(f\"Inserted {dates.shape[0]} dates into TimeDim\")\n",
    "\n",
    "# Get time_id mapping safely\n",
    "time_map_df = pd.read_sql_query(\"SELECT time_id, date FROM TimeDim\", conn)\n",
    "time_map_df.rename(columns={'date':'date_str'}, inplace=True)\n",
    "\n",
    "# Prepare merge keys\n",
    "df['date_str'] = df['InvoiceDate'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Drop any existing 'time_id' columns to avoid suffix issues\n",
    "if 'time_id' in df.columns:\n",
    "    df.drop(columns=['time_id'], inplace=True)\n",
    "\n",
    "# Merge df with time_map_df on 'date_str'\n",
    "df = df.merge(time_map_df, how='left', on='date_str')\n",
    "\n",
    "# Check for missing time_id after merge\n",
    "missing_time_ids = df['time_id'].isna().sum()\n",
    "if missing_time_ids > 0:\n",
    "    logger.warning(f\"{missing_time_ids} records have missing time_id after merge and will be skipped.\")\n",
    "\n",
    "# Insert sales records, skipping rows with missing foreign keys\n",
    "sales_inserted = 0\n",
    "for _, row in df.iterrows():\n",
    "    if pd.isna(row['CustomerID']) or pd.isna(row['time_id']):\n",
    "        continue  # Skip rows with missing keys\n",
    "    \n",
    "    cursor.execute('''\n",
    "        INSERT INTO SalesFact (\n",
    "            invoice_no, stock_code, description, quantity, unit_price, total_sales, customer_id, time_id\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    ''', (\n",
    "        row['InvoiceNo'],\n",
    "        row['StockCode'],\n",
    "        row['Description'],\n",
    "        int(row['Quantity']),\n",
    "        float(row['UnitPrice']),\n",
    "        float(row['TotalSales']),\n",
    "        int(row['CustomerID']),\n",
    "        int(row['time_id'])\n",
    "    ))\n",
    "    sales_inserted += 1\n",
    "\n",
    "conn.commit()\n",
    "logger.info(f\"Inserted {sales_inserted} sales records into SalesFact\")\n",
    "\n",
    "# Optional: Print counts to console\n",
    "for table in ['CustomerDim', 'TimeDim', 'SalesFact']:\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    print(f\"Table {table} has {count} records.\")\n",
    "\n",
    "conn.close()\n",
    "logger.info(\"Load step completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4449e3b8",
   "metadata": {},
   "source": [
    "# Full ETL Function: Extraction, Transformation, and Load (ETL)\n",
    "\n",
    "This function `run_etl()` performs the complete ETL process on the synthetic retail dataset, including:\n",
    "\n",
    "1. **Extraction**:\n",
    "   - Loads the CSV dataset into a Pandas DataFrame.\n",
    "   - Converts `InvoiceDate` to datetime format.\n",
    "   - Drops rows with missing critical values (`CustomerID`, `Description`).\n",
    "   - Removes duplicate rows.\n",
    "   - Ensures proper data types for numerical columns (`Quantity`, `UnitPrice`, `CustomerID`).\n",
    "   - Logs the number of rows processed and missing values.\n",
    "\n",
    "2. **Transformation**:\n",
    "   - Calculates a new column `TotalSales` as `Quantity * UnitPrice`.\n",
    "   - Removes outliers: rows with negative `Quantity` or non-positive `UnitPrice`.\n",
    "   - Filters data to include sales within the last year (Aug 13, 2024 – Aug 12, 2025).\n",
    "   - Creates a **customer summary table** with total purchases per customer and country information.\n",
    "   - Logs the number of rows processed at each step.\n",
    "\n",
    "3. **Load**:\n",
    "   - Connects to the SQLite database (`retail_dw.db`) and creates tables if they do not exist:\n",
    "     - `CustomerDim` (customer dimension table)\n",
    "     - `TimeDim` (time dimension table)\n",
    "     - `SalesFact` (sales fact table)\n",
    "   - Inserts unique customers into `CustomerDim`.\n",
    "   - Inserts unique invoice dates into `TimeDim`.\n",
    "   - Maps `InvoiceDate` to `time_id` for linking fact and dimension tables.\n",
    "   - Inserts processed sales records into `SalesFact`.\n",
    "   - Logs any missing `time_id` values and the number of rows inserted at each table.\n",
    "   - Closes the database connection.\n",
    "\n",
    "**Logging**:\n",
    "- All ETL steps are logged to both the console and a log file `etl_process.log`.\n",
    "- Logs include timestamps, log levels, and messages about rows processed, outliers removed, and warnings if any.\n",
    "\n",
    "This function ensures that the ETL pipeline is fully traceable, robust against missing or invalid data, and produces clean, ready-to-use data in the warehouse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec6934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import logging\n",
    "\n",
    "# =========================\n",
    "# Configure logging\n",
    "# =========================\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# File handler for log file\n",
    "file_handler = logging.FileHandler('etl_process.log', mode='w')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(file_formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Console handler for immediate output\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "console_handler.setFormatter(file_formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# =========================\n",
    "# Define ETL function\n",
    "# =========================\n",
    "def run_etl(csv_file='synthetic_retail_data.csv', db_file='retail_dw.db'):\n",
    "    # --- Extraction ---\n",
    "    logger.info(\"ETL Process started - Extraction step\")\n",
    "    \n",
    "    # Load CSV into DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Convert InvoiceDate to datetime\n",
    "    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='coerce')\n",
    "    \n",
    "    # Drop rows missing critical info\n",
    "    missing_before = df.shape[0]\n",
    "    df = df.dropna(subset=['CustomerID', 'Description'])\n",
    "    missing_after = df.shape[0]\n",
    "    logger.info(f\"Dropped {missing_before - missing_after} rows due to missing CustomerID or Description\")\n",
    "    \n",
    "    # Remove duplicate rows\n",
    "    duplicates_before = df.shape[0]\n",
    "    df = df.drop_duplicates()\n",
    "    duplicates_after = df.shape[0]\n",
    "    logger.info(f\"Removed {duplicates_before - duplicates_after} duplicate rows\")\n",
    "    \n",
    "    # Ensure proper numeric types\n",
    "    df['Quantity'] = pd.to_numeric(df['Quantity'], errors='coerce').fillna(0).astype(int)\n",
    "    df['UnitPrice'] = pd.to_numeric(df['UnitPrice'], errors='coerce').fillna(0.0).astype(float)\n",
    "    df['CustomerID'] = pd.to_numeric(df['CustomerID'], errors='coerce').astype('Int64')  # nullable int\n",
    "    \n",
    "    logger.info(f\"Extracted data shape after cleaning: {df.shape}\")\n",
    "    logger.info(f\"Missing values after cleaning:\\n{df.isna().sum()}\")\n",
    "    \n",
    "    # --- Transformation ---\n",
    "    logger.info(\"Transformation step started\")\n",
    "    \n",
    "    # Calculate TotalSales\n",
    "    df['TotalSales'] = df['Quantity'] * df['UnitPrice']\n",
    "    logger.info(\"Calculated TotalSales column\")\n",
    "    \n",
    "    # Remove outliers (Quantity < 0 or UnitPrice <= 0)\n",
    "    initial_shape = df.shape\n",
    "    df = df[(df['Quantity'] >= 0) & (df['UnitPrice'] > 0)]\n",
    "    logger.info(f\"Removed outliers: {initial_shape[0] - df.shape[0]} rows dropped\")\n",
    "    \n",
    "    # Filter for last year sales\n",
    "    start_date = pd.to_datetime('2024-08-13')\n",
    "    end_date = pd.to_datetime('2025-08-12')\n",
    "    df = df[(df['InvoiceDate'] >= start_date) & (df['InvoiceDate'] <= end_date)]\n",
    "    logger.info(f\"Filtered data for sales in last year: shape now {df.shape}\")\n",
    "    \n",
    "    # Create customer summary (dimension-like)\n",
    "    customer_summary = df.groupby('CustomerID').agg(\n",
    "        TotalPurchases=pd.NamedAgg(column='TotalSales', aggfunc='sum'),\n",
    "        Country=pd.NamedAgg(column='Country', aggfunc=lambda x: x.mode()[0] if not x.mode().empty else 'Unknown')\n",
    "    ).reset_index()\n",
    "    logger.info(f\"Created customer summary with {customer_summary.shape[0]} unique customers\")\n",
    "    \n",
    "    # --- Load ---\n",
    "    logger.info(\"Load step started\")\n",
    "    \n",
    "    # Connect to SQLite database\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create dimension and fact tables\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS CustomerDim (\n",
    "            customer_id INTEGER PRIMARY KEY,\n",
    "            country TEXT\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS TimeDim (\n",
    "            time_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            date TEXT UNIQUE,\n",
    "            year INTEGER,\n",
    "            month INTEGER,\n",
    "            day INTEGER\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS SalesFact (\n",
    "            sales_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            invoice_no TEXT,\n",
    "            stock_code TEXT,\n",
    "            description TEXT,\n",
    "            quantity INTEGER,\n",
    "            unit_price REAL,\n",
    "            total_sales REAL,\n",
    "            customer_id INTEGER,\n",
    "            time_id INTEGER,\n",
    "            FOREIGN KEY(customer_id) REFERENCES CustomerDim(customer_id),\n",
    "            FOREIGN KEY(time_id) REFERENCES TimeDim(time_id)\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    logger.info(\"Tables created or verified\")\n",
    "    \n",
    "    # Insert unique customers\n",
    "    customers = customer_summary[['CustomerID', 'Country']].rename(columns={'CustomerID': 'customer_id'})\n",
    "    for _, row in customers.iterrows():\n",
    "        cursor.execute('INSERT OR IGNORE INTO CustomerDim (customer_id, country) VALUES (?, ?)',\n",
    "                       (int(row['customer_id']), row['Country']))\n",
    "    conn.commit()\n",
    "    logger.info(f\"Inserted {customers.shape[0]} customers into CustomerDim\")\n",
    "    \n",
    "    # Insert unique dates into TimeDim\n",
    "    dates = df[['InvoiceDate']].drop_duplicates()\n",
    "    dates['year'] = dates['InvoiceDate'].dt.year\n",
    "    dates['month'] = dates['InvoiceDate'].dt.month\n",
    "    dates['day'] = dates['InvoiceDate'].dt.day\n",
    "    dates['date_str'] = dates['InvoiceDate'].dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    for _, row in dates.iterrows():\n",
    "        cursor.execute('INSERT OR IGNORE INTO TimeDim (date, year, month, day) VALUES (?, ?, ?, ?)',\n",
    "                       (row['date_str'], row['year'], row['month'], row['day']))\n",
    "    conn.commit()\n",
    "    logger.info(f\"Inserted {dates.shape[0]} dates into TimeDim\")\n",
    "    \n",
    "    # Map InvoiceDate to time_id\n",
    "    time_map_df = pd.read_sql_query(\"SELECT time_id, date FROM TimeDim\", conn)\n",
    "    time_map_df.rename(columns={'date':'date_str'}, inplace=True)\n",
    "    df['date_str'] = df['InvoiceDate'].dt.strftime('%Y-%m-%d')\n",
    "    df = df.merge(time_map_df, how='left', on='date_str')\n",
    "    \n",
    "    if df['time_id'].isnull().any():\n",
    "        missing_time_ids = df[df['time_id'].isnull()].shape[0]\n",
    "        logger.warning(f\"{missing_time_ids} sales records have missing time_id after merge\")\n",
    "    \n",
    "    # Insert sales into SalesFact\n",
    "    sales_inserted = 0\n",
    "    for _, row in df.iterrows():\n",
    "        if pd.isna(row['CustomerID']) or pd.isna(row['time_id']):\n",
    "            continue\n",
    "        cursor.execute('''\n",
    "            INSERT INTO SalesFact (\n",
    "                invoice_no, stock_code, description, quantity, unit_price, total_sales, customer_id, time_id\n",
    "            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', (row['InvoiceNo'], row['StockCode'], row['Description'], int(row['Quantity']),\n",
    "              float(row['UnitPrice']), float(row['TotalSales']), int(row['CustomerID']), int(row['time_id'])))\n",
    "        sales_inserted += 1\n",
    "    conn.commit()\n",
    "    logger.info(f\"Inserted {sales_inserted} sales records into SalesFact\")\n",
    "    \n",
    "    # Optional: log counts in each table\n",
    "    for table in ['CustomerDim', 'TimeDim', 'SalesFact']:\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        logger.info(f\"Table {table} has {count} records\")\n",
    "    \n",
    "    conn.close()\n",
    "    logger.info(\"ETL Process completed successfully\")\n",
    "    return df, customer_summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "163ccb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:05,523 - INFO - ETL Process started - Extraction step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:05,523 - INFO - ETL Process started - Extraction step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:05,546 - INFO - Dropped 35 rows due to missing CustomerID or Description\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:05,546 - INFO - Dropped 35 rows due to missing CustomerID or Description\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:05,553 - INFO - Removed 10 duplicate rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:05,553 - INFO - Removed 10 duplicate rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:05,562 - INFO - Extracted data shape after cleaning: (965, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:05,562 - INFO - Extracted data shape after cleaning: (965, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:05,571 - INFO - Missing values after cleaning:\n",
      "InvoiceNo      0\n",
      "StockCode      0\n",
      "Description    0\n",
      "Quantity       0\n",
      "InvoiceDate    0\n",
      "UnitPrice      0\n",
      "CustomerID     0\n",
      "Country        0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:05,571 - INFO - Missing values after cleaning:\n",
      "InvoiceNo      0\n",
      "StockCode      0\n",
      "Description    0\n",
      "Quantity       0\n",
      "InvoiceDate    0\n",
      "UnitPrice      0\n",
      "CustomerID     0\n",
      "Country        0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:05,575 - INFO - Transformation step started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:05,575 - INFO - Transformation step started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:05,581 - INFO - Calculated TotalSales column\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:05,581 - INFO - Calculated TotalSales column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:05,587 - INFO - Removed outliers: 15 rows dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:05,587 - INFO - Removed outliers: 15 rows dropped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:05,597 - INFO - Filtered data for sales in last year: shape now (476, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:05,597 - INFO - Filtered data for sales in last year: shape now (476, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:05,689 - INFO - Created customer summary with 100 unique customers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:05,689 - INFO - Created customer summary with 100 unique customers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:05,696 - INFO - Load step started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:05,696 - INFO - Load step started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:05,704 - INFO - Tables created or verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:05,704 - INFO - Tables created or verified\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:05,729 - INFO - Inserted 100 customers into CustomerDim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:05,729 - INFO - Inserted 100 customers into CustomerDim\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:05,987 - INFO - Inserted 476 dates into TimeDim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:05,987 - INFO - Inserted 476 dates into TimeDim\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:06,321 - INFO - Inserted 476 sales records into SalesFact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:06,321 - INFO - Inserted 476 sales records into SalesFact\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:06,324 - INFO - Table CustomerDim has 100 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:06,324 - INFO - Table CustomerDim has 100 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:06,327 - INFO - Table TimeDim has 351 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:06,327 - INFO - Table TimeDim has 351 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:06,331 - INFO - Table SalesFact has 3778 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:06,331 - INFO - Table SalesFact has 3778 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:06,334 - INFO - ETL Process completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 09:57:06,334 - INFO - ETL Process completed successfully\n"
     ]
    }
   ],
   "source": [
    "df, customer_summary = run_etl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debbdbd5",
   "metadata": {},
   "source": [
    "### ETL Function Explanation\n",
    "- The ETL process is encapsulated in a Python function called run_etl().\n",
    "\n",
    "**Key points:**\n",
    "\n",
    "***Extraction:***\n",
    "\n",
    "- Reads the CSV dataset.\n",
    "\n",
    "- Cleans the data by removing rows with missing CustomerID or Description and drops duplicates.\n",
    "\n",
    "- Ensures numeric columns have the correct data type.\n",
    "\n",
    "- Logs the number of rows dropped and the final shape of the dataset.\n",
    "\n",
    "***Transformation:***\n",
    "\n",
    "- Calculates a new column TotalSales.\n",
    "\n",
    "- Removes outliers where Quantity < 0 or UnitPrice <= 0.\n",
    "\n",
    "- Filters sales records to include only the last year.\n",
    "\n",
    "- Creates a customer_summary dataframe to aggregate total purchases and countries.\n",
    "\n",
    "- Logs all intermediate row counts and transformations.\n",
    "\n",
    "***Load:***\n",
    "\n",
    "- Creates the necessary SQLite tables (CustomerDim, TimeDim, SalesFact) if they do not exist.\n",
    "\n",
    "- Inserts cleaned customers and dates into their respective dimension tables.\n",
    "\n",
    "- Maps InvoiceDate to time_id and inserts the sales records into the fact table.\n",
    "\n",
    "- Logs the number of records inserted in each table.\n",
    "\n",
    "#### Important Note:\n",
    "\n",
    "- The function does not run automatically when imported. You must call it explicitly to execute the ETL process and see the results.\n",
    "\n",
    "Example of calling the function:\n",
    "\n",
    "```\n",
    "df, customer_summary = run_etl()\n",
    "```\n",
    "\n",
    "-After calling the function:\n",
    "\n",
    "   - df contains the fully cleaned and transformed sales dataset.\n",
    "\n",
    "   - customer_summary contains the aggregated customer-level data.\n",
    "\n",
    "   - Logs are written to both the console and the etl_process.log file, detailing all rows processed at each stage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
